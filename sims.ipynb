{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.linalg import subspace_angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8285323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowODE(nn.Module):\n",
    "    def __init__(self, r):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(r+1, 64), nn.Tanh(), nn.Linear(64, r))\n",
    "    def forward(self, t, z):\n",
    "        t = t if isinstance(t, float) else t.item()\n",
    "        tvec = torch.full_like(z[:, :1], t)\n",
    "        return self.net(torch.cat([z, tvec], 1))\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, data, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        r = data[\"rank\"]\n",
    "        self.K = data[\"K\"]\n",
    "        self.W0 = data[\"W0\"].to(device)\n",
    "        self.U  = data[\"U\"].to(device)\n",
    "        self.V  = data[\"V\"].to(device)\n",
    "        self.z0 = nn.Parameter(torch.zeros(r, device=device))\n",
    "        self.slow = SlowODE(r)\n",
    "        self.B = nn.Parameter(data[\"B\"].clone().to(device))\n",
    "        self.R = nn.Parameter(data[\"R\"].clone().to(device))\n",
    "        self.b_fixed = data[\"b\"].to(device)\n",
    "\n",
    "    def weights(self, T):\n",
    "        times = torch.arange(T, dtype=torch.float32, device=self.W0.device)\n",
    "        z = odeint(self.slow, self.z0.unsqueeze(0), times).squeeze(1)\n",
    "        return self.W0 + self.U @ torch.diag_embed(z) @ self.V.T\n",
    "\n",
    "    def rnn_cell(self, xk, vk, W):\n",
    "        return torch.tanh(W @ xk + (self.B * vk).squeeze() + self.b_fixed)\n",
    "\n",
    "    def forward(self, x0, v_seq, t_idx, Ws):\n",
    "        W = Ws[t_idx]; rec = dec = 0; x_prev = x0[0]\n",
    "        for k in range(self.K - 1):\n",
    "            x_pred = self.rnn_cell(x_prev, v_seq[k], W)\n",
    "            rec += (x_pred - x0[k+1]).pow(2).sum()\n",
    "            dec += (self.R @ x_pred - v_seq[k]).pow(2)\n",
    "            x_prev = x_pred\n",
    "        return rec + 0.1 * dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitDS(Dataset):\n",
    "    def __init__(self, x, v, days):\n",
    "        self.x = x\n",
    "        self.v = v\n",
    "        self.days = days\n",
    "        self.S = x.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.days) * self.S\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.days[idx // self.S]\n",
    "        s = idx % self.S\n",
    "        return self.x[t, s], self.v[t, s], t\n",
    "\n",
    "def train_destinode(data, train_days=[0, 1, 2], epochs=500, lr=1e-3, batch_size=4):\n",
    "    loader = DataLoader(\n",
    "        SplitDS(data[\"x\"], data[\"v\"], train_days),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    model = PINN(data, device=device).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        L = 0\n",
    "        for xb, vb, tb in loader:\n",
    "            xb, vb = xb.to(device), vb.to(device)\n",
    "            Ws = model.weights(data[\"T_rec\"])\n",
    "            loss = torch.stack([\n",
    "                model(xb[i], vb[i], tb[i], Ws)\n",
    "                for i in range(xb.size(0))\n",
    "            ]).mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            L += loss.item()\n",
    "        if ep % 100 == 0:\n",
    "            print(f\"  ep {ep}: {L/len(loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3196377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Two-Context Synthetic Data Generator\n",
    "# ============================================================\n",
    "# Timeline:  Days 0-3  → Expert A (drift only)\n",
    "#            Day  4    → Learning (context switch A→B)\n",
    "#            Days 5-9  → Expert B (drift only)\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_two_context(\n",
    "    drift_scale=1.0,   # controls |V|\n",
    "    learn_scale=1.0,   # controls |L|\n",
    "    T_rec=10, S=5, K=40, N=50, rank=3,\n",
    "    seed=0, device=\"cpu\"\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # ---- shared structural backbone (same across contexts) ----\n",
    "    W0 = 0.0005 * torch.randn(N, N, device=device)\n",
    "    Uf, _, Vtf = torch.linalg.svd(W0)\n",
    "    U, V = Uf[:, :rank], Vtf[:rank, :].T\n",
    "    B  = 0.05 * torch.randn(N, 1, device=device)\n",
    "    b  = 0.1  * torch.randn(N, device=device)\n",
    "    R  = 0.05 * torch.randn(1, N, device=device)\n",
    "\n",
    "    # ---- context A latents: smooth drift, days 0-3 ----\n",
    "    t_A = torch.linspace(0, torch.pi, 4, device=device)\n",
    "    z_A = drift_scale * torch.stack(\n",
    "        [torch.sin(t_A + torch.pi * i / rank) + 0.1 * t_A\n",
    "         for i in range(rank)], dim=1\n",
    "    )  # [4, rank]\n",
    "\n",
    "    # ---- counterfactual: linear extrapolation of A drift to day 4 ----\n",
    "    z_counterfactual_day4 = z_A[-1] + (z_A[-1] - z_A[-2])\n",
    "\n",
    "    # ---- learning jump: random direction, magnitude = learn_scale ----\n",
    "    torch.manual_seed(seed + 999)\n",
    "    jump_dir = torch.randn(rank, device=device)\n",
    "    jump_dir = jump_dir / jump_dir.norm()\n",
    "    z_B_start = z_A[-1] + learn_scale * jump_dir   # where B begins\n",
    "\n",
    "    # ---- context B latents: smooth drift, days 4-9 ----\n",
    "    t_B = torch.linspace(0, torch.pi, 6, device=device)\n",
    "    z_B_intrinsic = drift_scale * torch.stack(\n",
    "        [torch.sin(t_B + torch.pi * i / rank + 2.0) + 0.1 * t_B\n",
    "         for i in range(rank)], dim=1\n",
    "    )  # [6, rank]\n",
    "    z_B = z_B_start + (z_B_intrinsic - z_B_intrinsic[0])  # shift origin\n",
    "\n",
    "    # ---- full z_true: [10, rank] ----\n",
    "    z_true = torch.cat([z_A, z_B], dim=0)\n",
    "\n",
    "    # ---- build W(t), simulate RNN ----\n",
    "    Ws_true = W0 + torch.einsum(\"nr,tr,mr->tnm\", U, z_true, V)\n",
    "\n",
    "    x = torch.zeros(T_rec, S, K, N, device=device)\n",
    "    v = torch.zeros(T_rec, S, K, device=device)\n",
    "    for t in range(T_rec):\n",
    "        for s in range(S):\n",
    "            v_ts = 0.5 * torch.randn(K, device=device)\n",
    "            x_prev = 0.1 * torch.randn(N, device=device)\n",
    "            x[t, s, 0] = x_prev\n",
    "            for k in range(K - 1):\n",
    "                x_prev = torch.tanh(Ws_true[t] @ x_prev\n",
    "                                    + (B * v_ts[k]).squeeze() + b)\n",
    "                x[t, s, k + 1] = x_prev\n",
    "            v[t, s] = v_ts\n",
    "\n",
    "    return dict(\n",
    "        # backbone\n",
    "        W0=W0, U=U, V=V, B=B, b=b, R=R,\n",
    "        N=N, rank=rank, T_rec=T_rec, S=S, K=K,\n",
    "        # ground truth\n",
    "        z_true=z_true, z_A=z_A, z_B=z_B, Ws_true=Ws_true,\n",
    "        z_counterfactual_day4=z_counterfactual_day4,\n",
    "        # observables\n",
    "        x=x, v=v,\n",
    "        # params for bookkeeping\n",
    "        drift_scale=drift_scale, learn_scale=learn_scale,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4 Scenarios (each generates one full dataset)\n",
    "# ============================================================\n",
    "scenarios = {\n",
    "    \"high_V_high_L\": dict(drift_scale=2.0, learn_scale=2.0),\n",
    "    \"low_V_high_L\":  dict(drift_scale=0.3, learn_scale=2.0),\n",
    "    \"high_V_low_L\":  dict(drift_scale=2.0, learn_scale=0.3),\n",
    "    \"low_V_low_L\":   dict(drift_scale=0.3, learn_scale=0.3),\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for name, params in scenarios.items():\n",
    "    datasets[name] = generate_two_context(**params, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Extract V (drift) and L (learning) per neuron\n",
    "# ============================================================\n",
    "def extract_V_and_L(data, model):\n",
    "    \"\"\"\n",
    "    V_i = mean daily drift magnitude for neuron i (row norm of dW/dt)\n",
    "    L_i = learning residual magnitude for neuron i on day 4\n",
    "    \"\"\"\n",
    "    Ws_pred = model.weights(data[\"T_rec\"]).detach()\n",
    "\n",
    "    # V: average daily weight change during expert A (days 0-3)\n",
    "    dW_daily = torch.stack([Ws_pred[t] - Ws_pred[t-1] for t in range(1, 4)])\n",
    "    V_per_neuron = dW_daily.mean(0).norm(dim=1)  # [N]\n",
    "\n",
    "    # L: actual day-4 weights minus what drift model predicts\n",
    "    W_observed  = data[\"Ws_true\"][4].to(Ws_pred.device)\n",
    "    W_predicted = Ws_pred[4]  # counterfactual from drift-only model\n",
    "    L_per_neuron = (W_observed - W_predicted).norm(dim=1)  # [N]\n",
    "\n",
    "    return V_per_neuron.cpu().numpy(), L_per_neuron.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6733ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2×2 panel: one scatter per scenario\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, data) in enumerate(datasets.items()):\n",
    "    # --- train DestinODE on days 0-2 (expert A only) ---\n",
    "    model = train_destinode(data, train_days=[0, 1, 2])\n",
    "\n",
    "    Vi, Li = extract_V_and_L(data, model)\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(Vi, Li, c=\"k\", s=20, alpha=0.6)\n",
    "\n",
    "    # quadrant lines at median\n",
    "    ax.axvline(np.median(Vi), color=\"gray\", ls=\"--\", lw=0.8)\n",
    "    ax.axhline(np.median(Li), color=\"gray\", ls=\"--\", lw=0.8)\n",
    "\n",
    "    ax.set_title(name.replace(\"_\", \" \"), fontsize=11)\n",
    "    ax.set_xlabel(\"|V| drift\")\n",
    "    ax.set_ylabel(\"|L| learning\")\n",
    "\n",
    "    # annotate quadrant counts\n",
    "    n_hh = np.sum((Vi > np.median(Vi)) & (Li > np.median(Li)))\n",
    "    n_hl = np.sum((Vi > np.median(Vi)) & (Li <= np.median(Li)))\n",
    "    n_lh = np.sum((Vi <= np.median(Vi)) & (Li > np.median(Li)))\n",
    "    n_ll = np.sum((Vi <= np.median(Vi)) & (Li <= np.median(Li)))\n",
    "    ax.text(0.95, 0.95, f\"{n_hh}\", transform=ax.transAxes, ha=\"right\", va=\"top\")\n",
    "    ax.text(0.05, 0.95, f\"{n_lh}\", transform=ax.transAxes, ha=\"left\",  va=\"top\")\n",
    "    ax.text(0.95, 0.05, f\"{n_hl}\", transform=ax.transAxes, ha=\"right\", va=\"bottom\")\n",
    "    ax.text(0.05, 0.05, f\"{n_ll}\", transform=ax.transAxes, ha=\"left\",  va=\"bottom\")\n",
    "\n",
    "plt.suptitle(\"Drift vs Learning per neuron\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17c71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a9b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
